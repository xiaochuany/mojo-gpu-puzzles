# Summary

# Getting Started
- [Introduction](./introduction.md)
- [Mojo ğŸ”¥: The Best Way to Program GPUs]()
- [How to Use These Learning Puzzles]()
- [GPU Programming Resources]()

# Part I: GPU Fundamentals
- [Puzzle 1: Map](./puzzle_01/puzzle_01.md)
  - [ğŸ”° Traditional Approach](./puzzle_01/traditional.md)
  - [ğŸ’¡ Preview: Why LayoutTensor?](./puzzle_01/layout_tensor_preview.md)
- [Puzzle 2: Zip](./puzzle_02/puzzle_02.md)
- [Puzzle 3: Guards](./puzzle_03/puzzle_03.md)
- [Puzzle 4: 2D Map](./puzzle_04/puzzle_04.md)
    - [ğŸ“š Learn about LayoutTensor](./puzzle_04/introduction_layout_tensor.md)
    - [ğŸ“ LayoutTensor Version](./puzzle_04/puzzle_04_layout_tensor.md)
- [Puzzle 5: Broadcast](./puzzle_05/puzzle_05.md)
    - [ğŸ“ LayoutTensor Version](./puzzle_05/puzzle_05_layout_tensor.md)
- [Puzzle 6: Blocks](./puzzle_06/puzzle_06.md)
- [Puzzle 7: 2D Blocks](./puzzle_07/puzzle_07.md)
    - [ğŸ“ LayoutTensor Version](./puzzle_07/puzzle_07_layout_tensor.md)
- [Puzzle 8: Shared Memory](./puzzle_08/puzzle_08.md)
- [ğŸ¯ Bonus Challenges]()

# Part II: GPU Algorithms
- [Puzzle 9: Pooling](./puzzle_09/puzzle_09.md)
- [Puzzle 10: Dot Product](./puzzle_10/puzzle_10.md)
- [Puzzle 11: 1D Convolution](./puzzle_11/puzzle_11.md)
  - [ğŸ”° Simple Version](./puzzle_11/simple.md)
  - [â­ Complete Version](./puzzle_11/complete.md)
- [Puzzle 12: Prefix Sum](./puzzle_12/puzzle_12.md)
  - [ğŸ”° Simple Version](./puzzle_12/simple.md)
  - [â­ Complete Version](./puzzle_12/complete.md)
- [Puzzle 13: Axis Sum](./puzzle_13/puzzle_13.md)
- [Puzzle 14: Matrix Multiplication (MatMul)](./puzzle_14/puzzle_14.md)
    - [ğŸ”° Naive Version with Global Memory](./puzzle_14/naive.md)
    - [ğŸ“š Learn about Roofline Model]()
    - [ğŸ’« Shared Memory Version](./puzzle_14/shared_memory.md)
    - [âœ¨ Tiled Version](./puzzle_14/tiled.md)
    - [ğŸ“ Tiling with LayoutTensor]()

# Part III: Interfacing with Python via MAX Graph Custom Ops
- [Puzzle 15: 1D Convolution Op]()
- [Puzzle 16: Color Inversion Op]()
- [Puzzle 17: TopK Sampling Op]()
- [Puzzle 18: Softmax Op]()
- [Puzzle 19: Attention Op]()
- [ğŸ¯ Bonus Challenges]()

# Part IV: Advanced GPU Algorithms
- [Puzzle 20: 2D Convolution Op]()
- [Puzzle 21: 3D Average Pooling]()
  - [ğŸ“š Learn about 3D Memory Layout]()
  - [Basic Version]()
  - [LayoutTensor Version]()
- [Puzzle 22: 3D Convolution]()
  - [ğŸ“š Learn about 3D Convolution]()
  - [Basic Version]()
  - [Optimized Version]()
- [Puzzle 23: 3D Tensor Multiplication]()
  - [ğŸ“š Learn about Tensor Operations]()
  - [Basic Version]()
  - [LayoutTensor Version]()
- [Puzzle 24: Multi-Head Self-Attention]()
  - [ğŸ“š Learn about Attention Mechanisms]()
  - [Basic Version]()
  - [Optimized Version]()

# Part V: Performance Optimization Puzzles
- [Puzzle 25: Memory Coalescing]()
  - [ğŸ“š Learn about Memory Access Patterns]()
  - [Basic Version]()
  - [Optimized Version]()
- [Puzzle 26: Bank Conflicts]()
  - [ğŸ“š Learn about Shared Memory Banks]()
  - [Version 1: With Conflicts]()
  - [Version 2: Conflict-Free]()
- [Puzzle 27: Warp-Level Optimization]()
  - [ğŸ“š Learn about Warp Primitives]()
  - [Version 1: Shared Memory Reduction]()
  - [Version 2: Warp Shuffle Reduction]()

# Part VI: Real-world Application Puzzles
- [Puzzle 28: Image Processing Pipeline]()
  - [ğŸ“š Learn about Kernel Fusion]()
  - [Version 1: Separate Kernels]()
  - [Version 2: Fused Pipeline]()
- [Puzzle 29: Neural Network Layers]()
  - [ğŸ“š Learn about Layer Fusion]()
  - [Version 1: Basic Implementation]()
  - [Version 2: Optimized Implementation]()
- [Puzzle 30: Multi-Level Tiling]()
  - [ğŸ“š Learn about Cache Hierarchies]()
  - [Version 1: Single-Level MatMul]()
  - [Version 2: Multi-Level MatMul]()

# Part VII: Debug & Profile Puzzles
- [Puzzle 31: Race Condition Detective]()
  - [ğŸ“š Learn about Race Conditions]()
  - [Version 1: Find the Bug]()
  - [Version 2: Fix the Bug]()
- [Puzzle 32: Memory Optimization]()
  - [ğŸ“š Learn about Memory Management]()
  - [Version 1: Memory Leaks]()
  - [Version 2: Memory Planning]()

# Part VIII: Modern GPU Features
- [Puzzle 33: Dynamic Parallelism]()
  - [ğŸ“š Learn about Nested Parallelism]()
  - [Version 1: Flat Implementation]()
  - [Version 2: Nested Launch]()
- [Puzzle 34: Tensor Core Programming]()
  - [ğŸ“š Learn about Tensor Cores]()
  - [Version 1: Regular MatMul]()
  - [Version 2: Tensor Core MatMul]()
- [Puzzle 35: Multi-GPU Programming]()
  - [ğŸ“š Learn about Device Communication]()
  - [Version 1: Single GPU]()
  - [Version 2: Multi-GPU]()
